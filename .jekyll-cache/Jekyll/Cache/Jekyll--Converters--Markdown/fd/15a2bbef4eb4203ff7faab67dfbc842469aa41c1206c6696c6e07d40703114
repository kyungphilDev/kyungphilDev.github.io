I"Á<h3 id="nmf-ìŒìˆ˜ë¯¸í¬í•¨-í–‰ë ¬ë¶„í•´">NMF ìŒìˆ˜ë¯¸í¬í•¨ í–‰ë ¬ë¶„í•´</h3>

<p>Non-negative Matrix Factorization</p>

<p>SVDì™€ ê°™ì€ <strong>Matrix Factorization</strong> ê³¼ PCAì™€ ê°™ì€ <strong>ì°¨ì› ì¶•ì†Œ</strong>(dimension reduction)ì„ ì´ìš©í•œ ë°©ë²•ë“¤ì€ <strong>nonnegativityë¥¼</strong> ë³´ì¥í•  ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì— non-negative featuresë¥¼ ë‹¤ë£¨ëŠ” ë°ì´í„°ì—ì„œëŠ” ì‚¬ìš©í•˜ê¸°ì— ì í•©í•˜ì§€ ì•Šë‹¤.</p>

<p>ë”°ë¼ì„œ Non-negative ë°ì´í„°ëŠ” non-negative featureë¡œ ì„¤ëª…í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.</p>

<p>SVDëŠ” ì—¬ëŸ¬ ì¥ì ë“¤ì´ ìˆì§€ë§Œ, factorë“¤ì€ Interpretability(inputì´ë‚˜ parameterì— ë³€í™”ë¥¼ ì£¼ì—ˆì„ ë•Œ ì–´ë–¤ ê²°ê³¼ê°€ ë‚˜ì˜¬ ê²ƒì¸ì§€ ì˜ˆì¸¡í•  ìˆ˜ê°€ ì—†ë‹¤.)ë¥¼ ê°–ì¶”ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— data collectionì— ì–´ë– í•œ ê²ƒë„ ì•Œì•„ë‚¼ ìˆ˜ê°€ ì—†ë‹¤.</p>

<p>NMFë¥¼ í†µí•´ì„œ row-rank approximationì´ ê°€ëŠ¥í•˜ë‹¤. ì´ëŠ” noise filtering, feautre selection, compression, visualizationë“±ì— ì‘ìš©ì´ ëœë‹¤. NMFë¥¼ í†µí•´ì„œ basis elementì˜ setì„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ê°€ ìˆëŠ”ë°, ì´ëŠ” identificationê³¼ classification ì„ í†µí•´ì„œ unsupervised learning ê¸°ìˆ ì—ì„œ ì£¼ìš”í•œ ì—­í• ì„ í•œë‹¤.</p>

<h3 id="ill-posed-problem">ill-posed problem</h3>

<p>NMF(Lee and Seung)ë¥¼ ì œì•ˆí•  ë•Œ,</p>

<ul>
  <li>í•„ìš”í•œ ê°œë…: trace(ëŒ€ê°í•© ì—°ì‚°ì), norm, Frobenius norm(í–‰ë ¬ì˜ í¬ê¸° ê³„ì‚°), element-wise product OR Hadamard product(ì›ì†Œë³„ ê³±)</li>
</ul>

<blockquote>
  <p>ì°¸ê³  í‚¤ì›Œë“œ) k-means</p>
</blockquote>

<h3 id="matrix-factorization">Matrix Factorization</h3>

<p>Latent Factor Model</p>

<blockquote>
  <p>ì°¸ê³ ) ë‹¤ë¥¸ ëŒ€í‘œì  ì•Œê³ ë¦¬ì¦˜: Neighborhood Method</p>
</blockquote>

<p>ìœ ì €ì™€ ì•„ì´í…œë“¤ ì •ë³´ë¥¼ í† ëŒ€ë¡œ latent featureë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆëŠ” ë°©ë²•.</p>

<p>the goal of matrix factorization method is to predict missing entries.</p>

<ul>
  <li>user-based collaborative filtering</li>
  <li>item-based collaborative filtering</li>
</ul>

<h3 id="svd-íŠ¹ì´ì -ë¶„í•´">SVD íŠ¹ì´ì  ë¶„í•´</h3>

<p>Singual Vector Decomposition</p>

<h3 id="gradient-descent-ê²½ì‚¬í•˜ê°•ë²•">Gradient Descent ê²½ì‚¬í•˜ê°•ë²•</h3>

<p>í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ë¬¸ì œì—ì„œ ê²½ì‚¬í•˜ê°•ë²•ì´ ì‚¬ìš©ì´ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ë•Œ ë¯¸ë¶„ ê³„ìˆ˜ê°€ 0ì¸ ì§€ì ì„ ì°¾ì§€ ì•Šê³  ì™œ ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í•˜ëŠ”ì§€ì— ëŒ€í•œ ì˜ë¬¸ì ì´ ìƒê¸¸ ìˆ˜ê°€ ìˆë‹¤. ê²½ì‚¬í•˜ê°•ë²•ì€ ë‹¤ìŒì˜ ì„¸ê°€ì§€ ê²½ìš°ì— ìœ ìš©í•˜ë‹¤.</p>

<ul>
  <li>í•¨ìˆ˜ê°€ ë‹«íŒ í˜•íƒœê°€ ì•„ë‹Œ ê²½ìš°</li>
  <li>í•¨ìˆ˜ê°€ ë„ˆë¬´ ë³µì¡í•´ì„œ ë¯¸ë¶„ ê³„ìˆ˜ë¥¼ êµ¬í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°</li>
  <li>gradient descentë¥¼ êµ¬í˜„í•˜ëŠ”ê²Œ ë¯¸ë¶„ ê³„ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ì‰¬ìš´ ê²½ìš°</li>
  <li>ë°ì´í„° ì–‘ì´ ë„ˆë¬´ ë§ì•„ íš¨ìœ¨ì ì¸ ê³„ì‚°ì„ í•˜ê¸° ìœ„í•´</li>
</ul>

<blockquote>
  <p>ì°¸ê³ ìë£Œ)<br />
NMF : https://angeloyeo.github.io/2020/10/15/NMF.html<br />
kaggle_Nonnegative Matrix Factorization and Image Compression: https://www.kaggle.com/elenageminiani/nmf-and-image-compression</p>
</blockquote>
:ET