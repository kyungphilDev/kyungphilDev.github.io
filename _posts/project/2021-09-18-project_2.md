---
layout: posts
title: PCA, EVD
comments: true
category: project
tags: [ML, DL, recommendation system]
---

### PCA 주성분 분석(차원 축소)

Principal Component Analysis

> 참고) Auto Encoder

고차원의 데이터들을 정사영 시켜 차원을 낮춘다면,
어떤 벡터에 데이터들을 정사영시키는 것이 원래 데이터의 정보손실을 가장 최소화 할 수 있을까에 대한 방법론

- **Covariance Matrix 공분산 행렬**
<p align="center">
      <img src="https://user-images.githubusercontent.com/80669616/133917339-33ff64bc-1619-4fce-b807-d59da8c21a9d.png" width="300"><br>공분산 행렬의 계산
</p>
- 공분산 행렬을 통해 **각 feture들의 변동이 얼마나 닮아있는지** 를 알 수 있다.
- 기하학적 의미으로는 데이터를 어떻게 선형 변환하는가 이다.
- 공분산 행렬의 고유 벡터에 내적을 하는 것이 PCA 이다.
- 공분산 행렬의 고유벡터에 내적을 하면 **최대 Variance**로 나타난다.

### EVD 고윳값 분해

Eigen Value Decomposition

EVD는 기존의 선형변환을 세 가지 과정으로 분해하여 생각할 수 있게 도와줍니다.

<p align="center">
      <img src="https://user-images.githubusercontent.com/80669616/133918248-21086887-65f8-43f1-a5dc-ee210f1aac7d.png" width="900"><br>고윳값 분해 과정
</p>

흥미로운 점은 대칭행렬의 경우에는 다음과 같은 새로운 특징이 나타납니다.

<p align="center">
      <img src="https://user-images.githubusercontent.com/80669616/133918324-495e3637-4927-418a-a257-1546039221d0.png" width="130"><br>대칭행렬의 고윳값 분해
</p>
대칭 행렬은 고유벡터가 서로 직교하는 성질을 보이기 때문에 고유벡터들을 모아둔 행렬 V는 직교행렬이므로 V 대신에 Q로 나타냅니다.

- 대칭행렬의 기하학적 의미

대칭행렬은 행렬 Q를 통해 선형변환을 하게 되면, 기저 벡터의 길이는 1 이면서 직교하기 때문에 벡터 공간을 회전시키는 것과 같은 결과를 보여줍니다.

---

> 참고자료)  
> 주성분 분석(PCA) : https://angeloyeo.github.io/2019/07/27/PCA.html  
> 고윳값 분해(EVD) : https://angeloyeo.github.io/2020/11/19/eigen_decomposition.html
