---
layout: posts
title: NMF
comments: true
category: project
tags: [ML, DL, recommendation system]
---

### NMF 음수미포함 행렬분해

Non-negative Matrix Factorization

SVD와 같은 **Matrix Factorization** 과 PCA와 같은 **차원 축소**(dimension reduction)을 이용한 방법들은 **nonnegativity를** 보장할 수가 없기 때문에 non-negative features를 다루는 데이터에서는 사용하기에 적합하지 않다.

따라서 Non-negative 데이터는 non-negative feature로 설명하는 것이 좋다.

SVD는 여러 장점들이 있지만, factor들은 Interpretability(input이나 parameter에 변화를 주었을 때 어떤 결과가 나올 것인지 예측할 수가 없다.)를 갖추고 있지 않기 때문에 data collection에 어떠한 것도 알아낼 수가 없다.

NMF를 통해서 row-rank approximation이 가능하다. 이는 noise filtering, feautre selection, compression, visualization등에 응용이 된다. NMF를 통해서 basis element의 set을 만들어 낼 수가 있는데, 이는 identification과 classification 을 통해서 unsupervised learning 기술에서 주요한 역할을 한다.

### ill-posed problem

Lee and Seung 이 제안한 NMF 모델의 경우, A=WH의 decomposition이 unique 하다는 것을 보장하지 못한다.

- 필요한 개념: trace(대각합 연산자), norm, Frobenius norm(행렬의 크기 계산), element-wise product OR Hadamard product(원소별 곱)

---

> 참고 키워드) k-means, Neighborhood Method

- Matrix Factorization(Latent Factor Model)

유저와 아이템들 정보를 토대로 latent feature를 찾아낼 수 있는 방법.

the goal of matrix factorization method is to predict missing entries.

---

### SVD 특이점 분해

Singual Vector Decomposition

### Gradient Descent 경사하강법

함수의 최솟값을 찾는 문제에서 경사하강법이 사용이 된다. 그렇다면, 이때 미분 계수가 0인 지점을 찾지 않고 왜 경사하강법을 사용하는지에 대한 의문점이 생길 수가 있다. 경사하강법은 다음의 세가지 경우에 유용하다.

- 함수가 닫힌 형태가 아닌 경우
- 함수가 너무 복잡해서 미분 계수를 구하기 어려운 경우
- gradient descent를 구현하는게 미분 계수를 구하는 것보다 더 쉬운 경우
- 데이터 양이 너무 많아 효율적인 계산을 하기 위해

### 경사하강법의 직관적인 의미

함수 값이 낮아지는 방향을 독립 변수 값을 변형시키면서 최종적으로 최소 값을 찾도록 하는 독립 변수 값을 찾는 방법.

<p align="center">
  <img src="https://user-images.githubusercontent.com/80669616/133934592-dc05b914-63cb-41ac-b4d3-d24d49216779.png" width="200"><br>경사하강법 수식
</p>

그러나 Local Minima 문제가 있다는 문제점이 존재한다.

> 참고자료)  
> NMF : https://angeloyeo.github.io/2020/10/15/NMF.html  
> kaggle_Nonnegative Matrix Factorization and Image Compression: https://www.kaggle.com/elenageminiani/nmf-and-image-compression
